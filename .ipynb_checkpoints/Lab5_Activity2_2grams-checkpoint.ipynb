{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a spark context which will be used for mapper and reducer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process the lemmatizer file and make a hashMap of having keys as the lemma and its corresponsding values as the words\n",
    "\n",
    "lemmaWords = {}\n",
    "with open( \"new_lemmatizer.csv\", \"r\", encoding = 'utf-8') as f:\n",
    "    words = []\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        array = line.split(\",\")\n",
    "        lemma = array[0]\n",
    "        i = 1\n",
    "        while i < len(array):\n",
    "            if(array[i] != \"\" and array[i] != \"\\n\"):\n",
    "                words.append(array[i])\n",
    "            i+=1\n",
    "        lemmaWords[lemma] = words\n",
    "        words = []\n",
    "\n",
    "# Function to the 2gram word cooccurrence pairs\n",
    "\n",
    "def getTwoGramWordPairs(line):\n",
    "    if len(line.split(\">\")) > 1:\n",
    "        location = line.split(\">\")[0] + \">\"\n",
    "        words_array = re.sub(r'([^\\s\\w]|_)+', \"\",line.split(\">\")[1].strip()).split(\" \")\n",
    "        print(words_array)\n",
    "\n",
    "        for i in range(0, len(words_array)-2):\n",
    "            for j in range(i+1, len(words_array)-1):\n",
    "                if words_array[i] != \"\" and words_array[j] != \"\":\n",
    "                    if words_array[i] in lemmaWords and words_array[j] in lemmaWords:\n",
    "                        for lemma1 in lemmaWords[words_array[i]]:\n",
    "                            for lemma2 in lemmaWords[str(words_array[j])]:\n",
    "                                yield (lemma1+\"-\"+lemma2, location)\n",
    "                    elif words_array[i] in lemmaWords:\n",
    "                        lemma2 = words_array[j]\n",
    "                        for lemma1 in lemmaWords[words_array[i]]:\n",
    "                            yield (lemma1+\"-\"+lemma2, location)\n",
    "                    elif words_array[j] in lemmaWords:                \n",
    "\n",
    "                        lemma1 = words_array[i]\n",
    "                        for lemma2 in lemmaWords[words_array[j]]:\n",
    "                            yield (lemma1+\"-\"+lemma2, location)\n",
    "                    else:\n",
    "                        yield (words_array[i]+\"-\"+words_array[j], location)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLatMap is used as the mapper and reducebyKey is used as the reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time --- 14.132959127426147 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "words = sc.textFile(\"D:\\DIC\\Lab5\\Files\\*.tess\")\n",
    "\n",
    "words.flatMap(getTwoGramWordPairs).take(10)\n",
    "counts = words.flatMap(getTwoGramWordPairs).reduceByKey(lambda x, y: x + \" \" + y)\n",
    "\n",
    "print(\"Execution time --- %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the cooccurence pairs into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cooccurrence = counts.collect()\n",
    "f = open('2grams_output', 'w')\n",
    "\n",
    "for i in cooccurrence:\n",
    "    f.write(str(i)+\"\\n\")\n",
    "    \n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
